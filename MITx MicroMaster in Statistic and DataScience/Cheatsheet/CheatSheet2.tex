\documentclass[a4paper, 10pt,landscape]{article}
\usepackage{palatino}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{latexsym, marvosym}
\usepackage{pifont}
\usepackage{lscape}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage[bottom]{footmisc}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{pdfpages}
\usepackage{wrapfig}
\usepackage{enumitem}
\setlist[description]{leftmargin=0pt}
\usepackage{xfrac}

\usepackage[
            open,
            openlevel=2
            ]{bookmark}
\usepackage{relsize}
\usepackage{rotating}

 \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
    \copy0\kern-\wd0\mkern4mu\box0}} 
            
\newcommand{\noin}{\noindent}    
\newcommand{\logit}{\textrm{logit}} 
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}} 
\newcommand{\corr}{\textrm{Corr}} 
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\textrm{Bern}}
\newcommand{\Bin}{\textrm{Bin}}
\newcommand{\Beta}{\textrm{Beta}}
\newcommand{\Gam}{\textrm{Gamma}}
\newcommand{\Expo}{\textrm{Expo}}
\newcommand{\Pois}{\textrm{Pois}}
\newcommand{\Unif}{\textrm{Unif}}
\newcommand{\Geom}{\textrm{Geom}}
\newcommand{\NBin}{\textrm{NBin}}
\newcommand{\Hypergeometric}{\textrm{HGeom}}
\newcommand{\HGeom}{\textrm{HGeom}}
\newcommand{\Mult}{\textrm{Mult}}

\geometry{top=.4in,left=.2in,right=.2in,bottom=.4in}

\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

% -----------------------------------------------------------------------

\usepackage{titlesec}

\titleformat{\section}
{\color{blue}\normalfont\large\bfseries}
{\color{blue}\thesection}{1em}{}
\titleformat{\subsection}
{\color{violet}\normalfont\normalsize\bfseries}
{\color{violet}\thesection}{1em}{}
% Comment out the above 5 lines for black and white

\begin{document}

\raggedright
\footnotesize
\begin{multicols*}{3}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}


\section{Neural Network}
\begin{description}
	\item {\bf Motivation}: learn classifier and feature representation at the same time to improve performance. 
	\item {\bf Overcapacity}: Large models tend to be easier to learn because their units need to be adjusted so that they are, collectively sufficient to solve the task
	\item {\bf Backpropagation}~
		\begin{itemize}
			\item {\bf Model Setup}~
				$$Loss = C(a^L)$$
				$$a_j^l = f(\sum_k w_{jk}^l a_k^{l-1} + b_j^l)$$
				$$z_j^{l} = \sum_k w_{jk}^{l} a_k^{l-1} + b_j^l$$
					\begin{itemize}
						\item $b_j^l$ is the bias of the j-th neuron in the l-th layer
						\item $a_j^l$ is the activation of j-th neuron in the l-th layer
						\item $w_{jk}^l$ is the weight for the connection from the k-th neuron in the (l-1)-th layer to the j-th neuron in the l-th layer
					\end{itemize}
			\item {\bf Chain} define $\delta^l = \dfrac{\partial C}{\partial z_j^l}$
				$$\delta^{L} = \dfrac{\partial C}{\partial a_j^L}f'(z_j^L)$$
				$$\delta^l = \sum_k w_{kj}^{l+1}\delta_k^{l+1}f'(z_j^l)$$
				$$\dfrac{\partial C}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l$$
				$$\dfrac{\partial C}{\partial b_j^l} = \delta_j^l$$

		\end{itemize}
\end{description}


\section{Recurrent Network}
\begin{description}
	\item {\bf Motivation}: Model the sequence. How history is mapped to a feature vector (encoding) is alos part of the learning process
	\item {Difference with FF}
		\begin{itemize}
			\item Inputs received at each layer. 
			\item Layer number depend on lenght of sentence
			\item parameters of each layer are shared
		\end{itemize}
	\item {\bf Basic RNN}
		$$s_t  = tanh(W^{s,s}s_{t-1} + W^{s,x}x_t)$$
		\begin{itemize}
			\item Gradient can vanish or explode since the sequences are long and we apply transformation repeatedly. Can resolve by introducing gate
		\end{itemize}
	\item {\bf Simple Gated RNN}
		\begin{align*}
			g_t&=\text{sigmoid}\left(W^{g,s}s_{t-1}+W^{g,x}x_t\right)\\
			s_t&=\left(1-g_t\right)\odot s_{t-1}+g_t\odot\tanh(W^{s,s}s_{t-1}+W^{s,x}x_t)
		\end{align*}
	\item {\bf LSTM}
		\begin{alignat*}{2}
			f_t&=\text{sigmoid}\left(W^{f,h}h_{t-1}+W^{f,x}x_t\right)\quad&&{\text{forget gate}}\\
			i_t&=\text{sigmoid}\left(W^{i,h}h_{t-1}+W^{i,x}x_t\right) &&{\text{input gate}}\\
			o_t&=\text{sigmoid}\left(W^{o,h}h_{t-1}+W^{o,x}x_t\right) &&{\text{output gate}}\\
			c_t&=f_t\odot c_{t-1}+i_t\odot\tanh(W^{c,h}h_{t-1}+W^{c,x}x_t) \quad&&{\text{memory cell}}\\
			h_t&=o_t\odot\tanh(c_t)&&{\text{visible state}}
		\end{alignat*}	
	\item {\bf Markov Model}
		\begin{itemize}
			\item Consist of an UNK symbol for any word, <beg>, <end>
			\item {\bf Bigram Model} $\prod_{i=1} \mathbb{P}(w_i|w_{i-1})$
			\item ML estimation: $\widehat{\mathbb{P}}(w'|w) = \dfrac{count(w', w)}{\sum_{w_i} count(w, w_i)}$
		\end{itemize}
\end{description}



\section{Reinforcement Learning}
Goal: Learn a good policy with none or limited supervision
\begin{description}
	\item[MDP]~
		\begin{itemize}
			\item {\bf Transition} $T(s, a, s') = \mathbb{P}(s'|s, a)$
			\item {\bf Reward} $R(s, a, s')$. The reward of starting at s, taking action a and ending up at $s'$ 
		\end{itemize}
	\item[Utility]~
		\begin{itemize}
			\item {\bf Infinite Utility} $U[s_0,s_1,\dots]=\sum_{k=0}^{\infty}\gamma^kR(s_k).$
			\item An finite step utility function can depend on steps left (I.E. very few steps left may lead to risk taking behavior)
			\item Inifnite discounted utility make the utility depend on current step only
			\item Infinite discunted utility converge to $\dfrac{R_{min}}{1-\gamma}$
			\item {\bf Policy} $\pi(s)$ that assign an action $\pi$ to the state s
			\item {\bf Optimal Policy} $\pi_s^{\star} = argmax_{a_s}\mathbb{E}[U(a_s)]$
		\end{itemize}

	\item[Bellman Equations]~
		\begin{itemize}
			\item {\bf Value Function} $V(s)$ expected reward starting from state s and acting optimally ever since
				$$V(s) = \max_{a}Q(s, a) = Q(s,\pi^{\star}(s))$$
			\item {\bf Q Function} $Q(s, a)$ expected reward strating at s, acting with acting a and then acting optimally aftewrwards
				$$Q(s, a)=\sum_{s'}T(s,a,s')(R(s,a,s')+\gamma V(s'))$$
			\item{\bf Value Iteration} $V(s) = \max_{a}\sum_{s'}T(s,a,s')(R(s,a,s')+\gamma V(s')$
				$$V_{k+1}(s) = \max_{a}\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V_k(s')]$$
			\item For the iterations, can plug in the final value to the rhs and solve for value
			\item {\bf Q-value Iteration} 
				$$Q_{k+1}(s, a)=\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma \max_{a'}Q_{k}(s', a')]$$
			\item Value Iteration will finally converge as long as $\gamma <1$ (speed for one iterationL $O(|S|^2 |A|)$)
				$$V(s) - V_k(s) = \gamma (V(s) - v_{k-1}(s))$$
		\end{itemize}
		\item[Q-Value Iteration for RL] ~
			\begin{enumerate}
				\item Initialization: $Q(s,a)=0\;\forall s,a$
				\item Iterate until convergence:
				\begin{enumerate}
					\item Collect sample: $s,a,s',R(s,a,s')$
					\item Update:
					\begin{align*}
						Q_{i+1}(s,a)&\leftarrow\,\alpha\left[R(s,a,s')+\gamma\max\limits_{a'}Q_i(s',a')\right]+(1-\alpha)\,Q_i(s,a)\\
						&=Q_i(s,a)+\alpha\left[R(s,a,s')+\gamma\max\limits_{a'}Q_i(s',a')-Q_i(s,a)\right]
					\end{align*}
				\end{enumerate}
			\end{enumerate}
		\item[Exploration]~
			\begin{itemize}
				\item Exploitation (Optimal) $1-\epsilon$ 
				\item Exploitation (Random) $\epsilon$ should decay as get data
			\end{itemize}
		\item[NLP]~
			\begin{itemize}
				\item Difficulty arises from ambiguity intrinsic in natural languages
				\item {\bf Approaches}
					\begin{enumerate}
						\item Symbolic: Structured rules. Encode all required info into an elaborate knowledge representation
						\item Statistical: Infer language properties from large language samples.
					\end{enumerate}
				\item {\bf Word Embedding}
					\begin{enumerate}
						\item cosine similarity between words are used for a much sparse encoding
						\item Bag of words approach sums up all the word embeddings to encode hence does not capture sequence
					\end{enumerate}
			\end{itemize}
					
\end{description}





\end{multicols*}
\end{document}