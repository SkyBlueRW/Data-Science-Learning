\documentclass[a4paper, 10pt,landscape]{article}
\usepackage{palatino}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\usepackage{latexsym, marvosym}
\usepackage{pifont}
\usepackage{lscape}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage[bottom]{footmisc}
\usepackage{tikz}
\usetikzlibrary{shapes}
\usepackage{pdfpages}
\usepackage{wrapfig}
\usepackage{enumitem}
\setlist[description]{leftmargin=0pt}
\usepackage{xfrac}

\usepackage[
            open,
            openlevel=2
            ]{bookmark}
\usepackage{relsize}
\usepackage{rotating}

 \newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\setbox0\hbox{$#1#2$}%
    \copy0\kern-\wd0\mkern4mu\box0}} 
            
\newcommand{\noin}{\noindent}    
\newcommand{\logit}{\textrm{logit}} 
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}} 
\newcommand{\corr}{\textrm{Corr}} 
\newcommand{\N}{\mathcal{N}}
\newcommand{\Bern}{\textrm{Bern}}
\newcommand{\Bin}{\textrm{Bin}}
\newcommand{\Beta}{\textrm{Beta}}
\newcommand{\Gam}{\textrm{Gamma}}
\newcommand{\Expo}{\textrm{Expo}}
\newcommand{\Pois}{\textrm{Pois}}
\newcommand{\Unif}{\textrm{Unif}}
\newcommand{\Geom}{\textrm{Geom}}
\newcommand{\NBin}{\textrm{NBin}}
\newcommand{\Hypergeometric}{\textrm{HGeom}}
\newcommand{\HGeom}{\textrm{HGeom}}
\newcommand{\Mult}{\textrm{Mult}}

\geometry{top=.4in,left=.2in,right=.2in,bottom=.4in}

\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

% -----------------------------------------------------------------------

\usepackage{titlesec}

\titleformat{\section}
{\color{blue}\normalfont\large\bfseries}
{\color{blue}\thesection}{1em}{}
\titleformat{\subsection}
{\color{violet}\normalfont\normalsize\bfseries}
{\color{violet}\thesection}{1em}{}
% Comment out the above 5 lines for black and white

\begin{document}

\raggedright
\footnotesize
\begin{multicols*}{4}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% TITLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Statistical Model}
	\begin{description}
		\item[] $\left(E,\left(\mathbb{P}_\theta\right)_{\theta\in\Theta}\right)$
		\item[Parametric Model] Dim of $\Theta$ is finite. $\Theta\subseteq\mathbb{R}^d$
		\item[Identifiable Parameter]~
			The parameter $\theta$ is called {\it identifiable} if and only if the map $\theta\in\Theta\mapsto\mathbb{P}_\theta$ is injective (Veryfy by solve CDF/PMF and see if uniquely determined by $\theta$)., $$\theta\neq \theta'\quad\implies\quad\mathbb{P}_\theta\neq\mathbb{P}_{\theta'}$$
or equivalently, $$\mathbb{P}_\theta=\mathbb{P}_{\theta'}\quad\implies\quad\theta=\theta'.$$
		\item[Quantile] $F(q_{\alpha})=P(X \leq q_{\alpha}) = 1 - \alpha$
	\end{description}

\section{Convergence and Inequality}

Let $X_1,\dots,X_n$ be i.i.d. random variables with $\mathbb{E}\left[X\right]=\mu$ and $\var(X)=\sigma^2$.

\begin{description}
	\item[Law of Large Numbers] ~
	\begin{equation*}
	\overline{X}_n=\frac{1}{n}\sum_{i=1}^{n}X_i\quad\xrightarrow[n\rightarrow\infty]{\mathbb{P}, a.s.}\quad\mu.
	\end{equation*}
	\item[Central Limit Theorem] ~
	$$\sqrt{n}\left(\overline{X}_n-\mu\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}\left(0,\sigma^2\right).$$
	\item[Multi CLT]  Let $X_1,\dots,X_n\in\mathbb{R}^d$ be independent copies of a random vector $X$ such that $\mathbb{E}\left[X\right]=\mu$, $\cov\left(X\right)=\Sigma$, then
	$$\sqrt{n}\left(\overline{X}_n-\mu\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}_d\left(0,\Sigma\right)$$	
	\item[Hoeffding's Inequality] Let $X, X_1,\dots X_n$ be i.i.d. random variables such that $\mathbb{E}\left[X\right]=\mu$ and $X\in\left[a,b\right]$ almost surely. Then, \begin{equation*}
		\mathbb{P}\left(\left|\overline{X}_n-\mu\right|\geq\epsilon\right)\leq2e^{-\frac{2n\epsilon^2}{(b-a)^2}}\quad\forall\epsilon>0
	\end{equation*}
	\item[Markov Inequality] If $X \geq 0$ and $a > 0$, then $\mathbb{P} (X\geq a) \leq \frac{\mathbb{E}[X]}{a}$
	\item[Chebyshev Inequality] Variable is unlikely to be far from the mean. $\mathbb{P}(|X-\mu| \geq c) \leq \frac{\sigma^2}{c^2}$
\end{description}


\begin{description}
	\item[Almost Surely (a.s.) Convergence] ~
	\begin{equation*}
		T_n\xrightarrow[n\rightarrow\infty]{a.s.}T\iff\mathbb{P}\left[\left\{\omega:T_n(\omega)\xrightarrow[n\rightarrow\infty]{}T(\omega)\right\}\right]=1
	\end{equation*}
	\item[Convergence in Probability] ~
	\begin{equation*}
		T_n\xrightarrow[n\rightarrow\infty]{\mathbb{P}}T\iff\mathbb{P}\left(\left|T_n-T\right|\geq\epsilon\right)\xrightarrow[n\rightarrow\infty]{}0\quad\forall\epsilon>0
	\end{equation*}
	\item[Convergence in Distribution] ~
	\begin{equation*}
	T_n\xrightarrow[n\rightarrow\infty]{(d)}T\iff\mathbb{E}\left[f\left(T_n\right)\right]\xrightarrow[n\rightarrow\infty]{}\mathbb{E}\left[f\left(T\right)\right]
	\end{equation*}
	for all continuous and bounded function $f$.
\end{description}

\subsubsection{The Delta Method}
Let $\left(Z_n\right)_{n\geq1}$ be a sequence of random variables that satisfies $$\sqrt{n}\left(Z_n-\theta\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}\left(0,\sigma^2\right)$$
for some $\theta\in\mathbb{R}$ and $\sigma^2>0$ .
Let $g:\mathbb{R}\rightarrow\mathbb{R}$ be continuously differentiable at the point $\theta$. Then $$\sqrt{n}\left(g\left(Z_n\right)-g\left(\theta\right)\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}\left(0,\left(g'(\theta)\right)^2\sigma^2\right).$$

\subsection{Multivariate Delta Method}
Let $\left(T_n\right)_{n\geq1}$ sequence of random vectors in $\mathbb{R}^d$ such that
	$$\sqrt{n}\left(T_n-\theta\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}_d\left(0,\Sigma\right),$$
	for some $\theta\in\mathbb{R}^d$ and some covariance $\Sigma\in\mathbb{R}^{d\times d}$.
	Let $g:\mathbb{R}^d\rightarrow\mathbb{R}^k$ ($k\geq1$) be continuously differentiable at $\theta$. Then,
	$$\sqrt{n}\left(g\left(T_n\right)-g\left(\theta\right)\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}\left(0,\nabla g(\theta)^\intercal\Sigma\,\nabla g(\theta)\right),$$
	where $\nabla g(\theta)=\dfrac{\partial g(\theta)}{\partial\theta}=\left(\dfrac{\partial g_j}{\partial\theta_i}\right)_{1\leq i\leq d\atop
		 1\leq j\leq k}\in\mathbb{R}^{d\times k}$



\section{Estimation}
\begin{description}
	\item[Consistent Estimator]$$\hat{\theta}_n\xrightarrow[n\rightarrow\infty]{\mathbb{P}\; (\text{resp. } a.s.)}\theta\quad(\text{w.r.t. } \mathbb{P}).$$
	\item[Asymptotic Normal]$$\sqrt{n}\left(\hat{\theta}_n-\theta\right)\xrightarrow[n\rightarrow\infty]{(d)}\mathcal{N}\left(0,\sigma^2\right)$$
	\item[Jensen's Inequality] If the function $f(x)$ is convex, $$\mathbb{E}\left[f\left(X\right)\right]\geq f\left(\mathbb{E}\left[X\right]\right).$$
	\item[Total Variation]~
			$$\text{TV}\left(\mathbb{P}_\theta, \mathbb{P}_{\theta'}\right)=\max\limits_{A\subset E}\left|\mathbb{P}_\theta(A)-\mathbb{P}_{\theta'}(A)\right|$$
			$$\text{TV}\left(\mathbb{P}_\theta,\mathbb{P}_{\theta'}\right)=\frac{1}{2}\sum_{x\in E}\left|p_\theta(x)-p_{\theta'}(x)\right|$$
			$$\text{TV}\left(\mathbb{P}_\theta,\mathbb{P}_{\theta'}\right)=\frac{1}{2}\int\left|f_\theta(x)-f_{\theta'}(x)\right|dx$$
	\item[Kullback-Leibler(KL) Divergence]: positive and definite (0 means same distribution), but not meet triagular inequality and symmetrical
	\[\text{KL}\left(\mathbb{P}_\theta,\mathbb{P}_{\theta'}\right)=
	\begin{cases}
		\sum\limits_{x\in E}p_\theta(x)\log\left(\dfrac{p_\theta(x)}{p_{\theta'}(x)}\right)\qquad\text{if $E$ is discrete}\\[10pt]
		{\displaystyle\int_{E}}f_\theta(x)\log\left(\dfrac{f_\theta(x)}{f_{\theta'}(x)}\right)dx\qquad\text{if $E$ is continuous}
	\end{cases}\]

\end{description}

\subsection{MLE}
\begin{description}
	

	\item[MLE estimator] 
		$$\hat{\theta}_n^\text{MLE}=\argmax\limits_{\theta\in\Theta}L\left(X_1,\dots,X_n,\theta\right),$$
		$$ = \argmax\limits_{\theta\in\Theta} \sum_i^n log(f_{\theta}(X_i))$$

	\item[Fisher Information] On average how curved is the log-likelihood function
		
		$$\ell(\theta)=\log L_1\left(X,\theta\right),\quad\theta\in\Theta\subset\mathbb{R}^d.$$
		Assume that $\ell$ is a.s. twice differentiable. Under some regularity conditions, the Fisher information of the statistical model is defined as
		$$I(\theta)=\mathbb{E}\left[\nabla\ell(\theta)\nabla\ell(\theta)^\intercal\right]-\mathbb{E}\left[\nabla\ell(\theta)\right]\mathbb{E}\left[\nabla\ell(\theta)\right]^\intercal=-\mathbb{E}\left[\mathbb{H}\ell(\theta)\right].$$
		If $\Theta\subset\mathbb{R}$, we get
		$$I(\theta)=\var\left[\ell'(\theta)\right]=-\mathbb{E}\left[\ell''(\theta)\right].$$
	
	\item[Asymptotical Normality]~
		\begin{enumerate}
		\item The parameter is identifiable.
		\item For all $\theta\in\Theta$, the support of $\mathbb{P}_\theta$ does not depend on $\theta$.
		\item $\theta^*$ is not on the boundary of $\Theta$.
		\item $I(\theta)$ is invertible in a neighborhood of $\theta^*$.
		\item A few more technical conditions.
		\end{enumerate}
		Then, $\hat{\theta}_n^\text{MLE}$ satisfies
		\begin{itemize}
			\item $\hat{\theta}_n^\text{MLE}\quad\xrightarrow[n\rightarrow\infty]{\mathbb{P}}\quad\theta^*$ w.r.t. $\mathbb{P}_{\theta^*}$;
			\item $\sqrt{n}\left(\hat{\theta}_n^\text{MLE}-\theta^*\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}_d\left(0,I^{-1}(\theta^*)\right)$ w.r.t. $\mathbb{P}_{\theta^*}$.
		\end{itemize}		
	
	\item[EM algorithm]~
		\begin{enumerate}
		\item[] Randomly initialize all parameters $\theta$ for latent variable Z and observable variable X
		\item {\bf E-step:} (Complete data by replacing $Z_i$ with conditional expectation  $\mathbb{E}[Z_i|X_i]$ when $Z_i$ is Bernoulli = $\mathbb{P}(Z_i=1|X_i)$)
			$$p(j|i)=\dfrac{p_j\mathcal{N}\left(\mathbf{x}^{(i)};\mu^{(j)},\sigma^2_j\mathbf{I}\right)}{p\left(\mathbf{x}|\theta\right)}$$
			where likelihood $p(\mathbf{x}|\theta)=\sum\limits_{j=1}^{K}p_j\mathcal{N}\left(\mathbf{x}^{(i)};\mu^{(j)},\sigma^2_j\mathbf{I}\right)$
		\item {\bf M-step:} (Plug $Z_i$ in likelihood and optimize with respect to parameter of X)
			$$\widehat{n}_j=\sum_{i=1}^{n}p(j|i), \widehat{p}_j=\dfrac{\widehat{n}_j}{n}$$
			$$\widehat{\mu}^{(j)}=\dfrac{1}{\widehat{n}_j}\sum_{i=1}^{n}p(j|i)\,\mathbf{x}^{(i)}$$
			$$\widehat{\sigma}^2_j=\dfrac{1}{\widehat{n}_jd}\sum_{i=1}^{n}p(j|i) (||\mathbf{x}^{(i)}-\mu^{(j)} ||)^2$$
		\end{enumerate}

\subsection{M-estimation}
	\item[M-estimation]
		\begin{enumerate}
			
			\item Find the loss function $\rho:E\times\mathcal{M}\rightarrow\mathbb{R}$  where $\mathcal{M}$ is the set of all possible values for the unknown $\mu$, such that $$Q(\mu):=\mathbb{E}\left[\rho\left(X_1,\mu\right)\right]$$ achieves its minimum at $\mu=\mu^*$.
			\item Estimator is then $\widehat{\mu}=\argmin\limits_{\mu}\dfrac{1}{n} \sum_i^n \rho(X_i, \mu)$
		\end{enumerate}

		\begin{itemize}
		\item If $E=\mathcal{M}=\mathbb{R}$ and $\rho(x,\mu)=(x-\mu)^2,$ for all $x,\mu\in\mathbb{R}$: $\mu^*=\mathbb{E}\left[X\right]$.
		\item If $E=\mathcal{M}=\mathbb{R}^d$ and $\rho(x,\mu)=\lVert x-\mu\rVert_2^2$, for all $x,\mu\in\mathbb{R}^d$: $\mu^*=\mathbb{E}\left[X\right]\in\mathbb{R}^d$.
		\item If $E=\mathcal{M}=\mathbb{R}$ and $\rho(x,\mu)=|x-\mu|$, for all $x,\mu\in\mathbb{R}$: $\mu^*$ is a {\bf median} of $\mathbb{P}$.
		\item If $E=\mathcal{M}=\mathbb{R}$, $\alpha\in(0,1)$ is fixed and $\rho(x,\mu)=C_\alpha(x-\mu)$, for all $x,\mu\in\mathbb{R}$: $\mu^*$ is a $\alpha$-quantile of $\mathbb{P}$.
			$$C_\alpha=\begin{cases}
			-(1-\alpha)x&\text{if }x<0\\
			\alpha x&\text{if }x\geq0.
			\end{cases}$$
		\end{itemize}

\subsection{Method of Moment Estimator}
	\item[Moment Generating Function]~
			$$M_X(t) = \mathbb{E}{e^{[tX]}}$$
			$$\mathbb{E}[X^k] = \dfrac{d^k}{dt^k}M_X(t)|_{t=0}$$

		\begin{description}
			\item[Population Moments] Let $m_k(\theta)=\mathbb{E}_\theta\left[X_1^k\right]$, $1\leq k\leq d$.
			\item[Empirical Moments] Let $\hat{m}_k=\overline{X_n^k}=\frac{1}{n}{\displaystyle\sum_{i=1}^{n}X_i^k}$, $1\leq k\leq d$.
			$$\left(\hat{m}_1,\dots,\hat{m}_d\right)\quad\xrightarrow[n\rightarrow\infty]{\mathbb{P}/a.s.}\quad\left(m_1(\theta),\dots,m_d(\theta)\right)$$
		\end{description}
		
	\item[Moments Estimator]
		\begin{description}
			\item Let
			\begin{align*}
			M:\Theta&\rightarrow\mathbb{R}^d\\
				\theta&\mapsto M(\theta)=\left(m_1(\theta),\dots,m_d(\theta)\right)
			\end{align*}
			\item Assume $M$ is one-to-one: $$\theta=M^{-1}\left(m_1(\theta),\dots,m_d(\theta)\right)$$
			\item[Moments estimator of $\theta$:] ~
			$$\widehat{\theta}_n^\text{\;MM}=M^{-1}\left(\widehat{m}_1,\dots,\widehat{m}_d\right)$$
		\end{description}
	
	\item[Generalized Method of Moment]~
		$$\sqrt{n}\left(\widehat{\theta}_n^\text{\;MM}-\theta\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}\left(0,\Gamma(\theta)\right),$$
	where $\Gamma(\theta)=\left[\dfrac{\partial M^{-1}}{\partial\theta}M(\theta)\right]^\intercal\Sigma(\theta)\left[\dfrac{\partial M^{-1}}{\partial\theta}M(\theta)\right]$
\end{description}

\section{Confidence Interval}
	\begin{description}
		\item[CI]: $\mathcal{I} = [L(X_1,...,X_n), U(X_1,...,X_n))]$
		\item[CI of level $1-\alpha$] $$\mathbb{P}_\theta\left[\mathcal{I}\ni\theta\right]\geq1-\alpha,\quad\forall\theta\in\Theta$$
		\item[CI of asymptotical level]  $$\lim\limits_{n\rightarrow\infty}\mathbb{P}_\theta\left[\mathcal{I}\ni\theta\right]\geq1-\alpha,\quad\forall\theta\in\Theta.$$
		\item[Procedures to a CI] ~
			\begin{enumerate}
				\item  Start from a pivot statistic (non-asymptotic) or CLT (asymptotic)
				\item Solve for $\mathbb{P}(\theta \in [\widehat{\theta}-s, \widehat{\theta}+s]) = 1-\alpha$
				\item  Two side symmetrical $$\mathcal{I} = [\widehat{\theta} - \dfrac{\sigma q_{\alpha / 2}}{\sqrt{n}}, \widehat{\theta} + \dfrac{\sigma q_{\alpha / 2}}{\sqrt{n}}]$$
					\begin{enumerate}
						\item Conservative bound: use known bound on $\sigma$
						\item Solve: solve equation
						\item Plug-in: plug a consistent estimator of $\sigma$ 
					\end{enumerate}
			\end{enumerate}
	\end{description}

\section{Hypotheses Testing}
$$\psi=\mathds{1}\{|T_n|>q_{\alpha / 2}\}=\mathds{1}\{T_n >q_{\alpha}\}=\mathds{1}\{T_n <-q_{\alpha}\}$$ Yes or No answer against 2 disjoint regions (both should be subsets of parameter space)
	\begin{itemize}
		\item {\bf Rejection region} of a test $\psi$: $R_\psi=\left\{x\in E^n:\psi(x)=1\right\}.$
		\item {\bf Power Function}: $\beta(\theta) = \mathbb{P}_{\theta}[\psi = 1]$ 
		\item {\bf Type I Error}: If $\theta \in \Theta_0$ (Given Null Reject Null; Reject wrongly)
			$$ \mathbb{P}_{\theta}[Type I of \psi] = \beta(\theta)$$
		\item{\bf Type II Error}: If $\theta \in \Theta_0$ (Given Alter not Reject Null)
			$$\mathbb{P}_{\theta}[Type II of \psi] = 1 - \beta(\theta)$$
		\item{\bf Level} (upper bound on Type I error): A test $\psi$ has level $\alpha$ if 
			\begin{enumerate}
				\item Non-Asymptotic: $\max_{\theta \in \Theta_0} \mathbb{P}_{\theta}[\psi = 1] <= \alpha$
				\item Asymptotic:  $\lim\limits_{n\rightarrow\infty} \max_{\theta \in \Theta_0} \mathbb{P}_{\theta}[\psi = 1] <= \alpha$
			\end{enumerate}
		\item{\bf Test from CI}~
			 Given a CI at level $1-\alpha$ I = [A, B], $\psi = 1 [\theta_0 \notin [A, B]]$ is a test at level $1-\alpha$

		\item {\bf p-value}~
			The (asymptotic) p-value of a test $\psi$ is the smallest (asymptotic) level $\alpha$ at which $\psi$ rejects $H_0$ \\
			p-value = $\mathbb{P}(Z > T_n^{obs})$	
	\end{itemize}

\subsection{Parametric Test}
	\begin{description}
		\item[Wald's Test]~
			\begin{itemize}
				If an estimator is both consistent and asymptotically normal. Then we can define test with following test statistic $W=\dfrac{\widehat{\theta} - \theta_0}{\sqrt{\widehat{var(\widehat{\theta})}}}$. 
				\item require Slusky for replacing $\sigma$
				\item $\widehat{var(\widehat{\theta})}$ can be any consistent variance estimator of $\widehat{\theta}$
				\item For MLE estimator it equals $W=\sqrt{n I(\widehat{\theta}^{MLE})}(\widehat{\theta}^{MLE} - \theta_0)$
				\item 2-sample Wald-Test $$\dfrac{(\widehat{\mu_1} - \widehat{\mu_2}) - (\mu_1 - \mu_2)}{\sqrt{\dfrac{\widehat{\sigma_1}^2}{n_1} + \dfrac{\widehat{\sigma_2}^2}{n_2}}}$$
			\end{itemize}
		\item[Likelihood Test]~
			
			Consider an i.i.d. sample $X_1,\dots,X_n$ with statistical model $\left(E,\left(\mathbb{P}_\theta\right)_{\theta\in\Theta}\right)$, where $\Theta\subseteq\mathbb{R}^d$ ($d\geq1$). Suppose the null hypothesis has the form
			$$H_0: \left(\theta_{r+1},\dots,\theta_{d}\right)=\left(\theta_{r+1}^{(0)},\dots,\theta_{d}^{(0)}\right),$$
			for some fixed and given numbers $\theta_{r+1}^{(0)},\dots,\theta_{d}^{(0)}$.
			\item Let $$\widehat{\theta}_n=\argmax\limits_{\theta\in\Theta}\ell_n(\theta)\qquad\text{(MLE)}$$
			and $$\widehat{\theta}_n^c=\argmax\limits_{\theta\in\Theta_0}\ell_n(\theta)\qquad\text{({\it constrained MLE})}$$
			where $\Theta_0=\left\{\theta\in\Theta:\left(\theta_{r+1},\dots,\theta_{d}\right)=\left(\theta_{r+1}^{(0)},\dots,\theta_{d}^{(0)}\right)\right\}$
			\item Test statistic: ~
			$$T_n=2\left(\ell_n\left(\hat{\theta}_n\right)-\ell_n\left(\hat{\theta}_n^c\right)\right).$$
			\item Wilk's Theorem Assume $H_0$ is true and the MLE technical conditions are satisfied. Then,
			$$T_n\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\chi_{d-r}^2$$
			\item Likelihood ratio test with asymptotic level $\alpha\in(0,1)$:
			$$\psi=\mathds{1}\left\{T_n>q_\alpha\right\},$$
			where $q_\alpha$ is the ($1-\alpha$)-quantile of $\chi_{d-r}^2$.

		\item[On sample T test]~
			Works for expected value of Gaussian $X_i$ and small sample. In general, Wald test leads to smaller p-value		

			For a positive integer $d$, the Student's T distribution with $d$ degrees of freedom (denoted by $t_d$) is the law of the random variable $\dfrac{Z}{\sqrt{V/d}}$, where $Z\sim\mathcal{N}(0,1)$, $V\sim\chi_d^2$ and $Z\independent V$.
			
			\item $$T_n=\sqrt{n}\dfrac{\overline{X}_n - \mu}{\sqrt{\widetilde{S}_n}}=\dfrac{\sqrt{n}\dfrac{\overline{X}_n-\mu}{\sigma}}{\sqrt{\dfrac{\widetilde{S}_n}{\sigma^2}}} \sim t_{n-1}$$, where $\widetilde{S}_n$ is the unbiased estimator

		\item[Two sample T test]~

			$$\dfrac{\overline{X}_n-\overline{Y}_m-\left(\Delta_d-\Delta_c\right)}{\sqrt{\dfrac{\widehat{\sigma}_d^2}{n}+\dfrac{\widehat{\sigma}_c^2}{m}}}\sim t_N$$
			
			$$N=\dfrac{\left(\dfrac{\widehat{\sigma}_d^2}{n}+\dfrac{\widehat{\sigma}_c^2}{m}\right)^2}{\dfrac{\widehat{\sigma}_d^4}{n^2(n-1)}+\dfrac{\widehat{\sigma}_c^4}{m^2(m-1)}}\geq\min(n,m)$$

		\item[Multiple Test]~
			\begin{itemize}
				\item Family-wise error rate (FWER) = prob of making at least one false discovery (type I)
				\item False discovery rate (FDR) = expected fraction of false discovery among all significant results
				\item FDR $<=$ FWER
				\item Bonferroni Correction to control FWER
					$$p^i < \dfrac{\alpha}{m}$$
				\item BH to control FDR
					\begin{enumerate}
						\item order p-value $P_1 < P_2 < ... < P_N$
						\item find max k such that $P_i <= \dfrac{k}{m} \alpha$
						\item reject all of $H_0^1$, ...., $H_0^k$
					\end{enumerate}
			\end{itemize}
\end{description}		

\subsection{Nonparametric Testing}
	\begin{description}
		\item[$\chi$ Test]~
			when $H_0$ hold $$T_n=n\sum_{j=1}^{K}\dfrac{\left(\widehat{\mathbf{p}}_j-\mathbf{p}_j^0\right)^2}{\mathbf{p}_j^0}\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\chi_{K-1}^2$$
		\item[$\chi$ Test for Family of Distribution]~
			$$T_n=n\sum_{j=1}^{K}\dfrac{\left(\widehat{\mathbf{p}}_j- f_{\widehat{\theta}}(j) \right)^2}{f_{\widehat{\theta}}(j)}\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\chi_{K-d-1}^2$$
			
			d is the dim of parameter space
		\item[Empirical CDF]~
			\begin{align*}
				F_n(t)&=\dfrac{1}{n}\sum_{i=1}^{n}\mathds{1}\left\{X_i\leq t\right\}\\
				&=\dfrac{\#\{i=1,\dots,n:X_i\leq t\}}{n},\quad\forall t\in\mathbb{R}.
			\end{align*}
		\item[Consistency] $F_n(t)\quad\xrightarrow[n\rightarrow\infty]{a.s.}\quad F(t).$
		\item[Glivenko-Cantelli Theorem] ~ $$\sup\limits_{t\in\mathbb{R}}\left|F_n(t)-F(t)\right|\quad\xrightarrow[n\rightarrow\infty]{a.s.}\quad0$$
		\item[Asymptotic Normality] $\sqrt{n}\left(F_n(t)-F(t)\right)\quad\xrightarrow[n\rightarrow\infty]{(d)}\quad\mathcal{N}\left(0,F(t)\left(1-F(t)\right)\right)$
		\item[Donsker's Theorem] $$\sqrt{n}\sup\limits_{t\in\mathbb{R}}\left|F_n(t)-F(t)\right|\quad\xrightarrow[n\rightarrow\infty]{a.s.}\quad\sup\limits_{0\leq t\leq 1}\left|\mathbf{B}(t)\right|,$$
	where $\mathbf{B}(t)$ is a Brownian bridge on $[0,1]$. 
		\item[Kolmogorov-Smirnov Test]~
			\begin{description}
				\item Let $T_n=\sup\limits_{t\in\mathbb{R}}\sqrt{n}\left|F_n(t)-F(t)\right|$. By Donsker's theorem, if $H_0$ is true, then $T_n\xrightarrow[n\rightarrow\infty]{(d)}Z$, where $Z$ has a known distribution (supremum of the absolute value of a Brownian bridge).
				\item[KS test with asymptotic level $\alpha$:] ~
				$$\delta_\alpha^\text{KS}=\mathds{1}\left\{T_n>q_\alpha\right\}$$
				where $q_\alpha$ is the ($1-\alpha$)-quantile of $Z$.
				\item Let $X_{(1)}\leq X_{(2)}\leq\dots\leq X_{(n)}$ be the reordered sample. The expression for $T_n$ reduces to
				$$T_n=\sqrt{n}\max\limits_{i=1,\dots,n}\left\{\max\left(\left|\dfrac{i-1}{n}-F^0\left(X_{(i)}\right)\right|,\left|\dfrac{i}{n}-F^0\left(X_{(i)}\right)\right|\right)\right\}.$$
				KS table is for $\dfrac{T_n}{\sqrt{n}}$
			\end{description}	
		\item[Kolmogorov-Lilliefors Test]~
			\begin{description}		
				\item We want to test if $X$ has a Gaussian distribution with unknown parameters. In this case, Donsker's theorem is {\it no longer valid}. Instead, we compute the quantiles for the test statistic
				$$\sqrt{n}\sup\limits_{t\in\mathbb{R}}\left|F_n(t)-\Phi_{\hat{\mu},\hat{\sigma}^2}(t)\right|$$
				where $\hat{\mu}=\overline{X}_n$, $\hat{\sigma}^2=S_n^2$ and $\Phi_{\hat{\mu},\hat{\sigma}^2}(t)$ is the CDF of $\mathcal{N}\left(\hat{\mu},\hat{\sigma}^2\right).$
				\item They do not depend on unknown parameters.
				\item should compare $\dfrac{F_n}{\sqrt{n}}$ with the table for both tests
				\item Kolmogorov-Smirnov has a greater prob of rejection
				\item Both Kolmogorov-Smirnov and Kolmogorov-Lilliefors test are non-asymptotic (statistics are pivot even for small n)
			\end{description}
		\item[QQ plot]~
			\begin{itemize}
				\item Check if the points
				$$\left(F^{-1}(\tfrac{1}{n}),F_n^{-1}(\tfrac{1}{n})\right),\dots,\left(F^{-1}(\tfrac{n-1}{n}),F_n^{-1}(\tfrac{n-1}{n})\right)$$
				are near the line $y=x.$
				\item $F_n$ is not technically invertible but we define
				$$F_n^{-1}(\tfrac{i}{n})=X_i,$$
				the $i$\textsuperscript{th} largest observation.
				\item Right heavy tail (above). Left heavy tail (below)
			\end{itemize}		
	\end{description}

\section{Bayesian Stat}
$$\pi\left(\theta|X_1,\dots,X_n\right)\propto\pi(\theta)L_n(X_1,\dots,X_n|\theta),\quad\forall\theta\in\Theta$$

	\begin{description}
		\item[Maximum a posteriori probability (MAP)]  The MAP estimate, $\hat{\theta}$, is the value at which the posterior distribution is maximum:  $$f_{\Theta|X}(\theta^*|x)=\max\limits_{\theta}f_{\Theta|X}(\theta|x).$$
		\item[Least Mean Squares (LMS)] The LMS estimate is the conditional expectation of the posterior distribution: $$\hat{\theta}=\mathds{E}\left[\Theta|X=x\right].$$
		\item[Linear Least Mean Squares LLMS]~
			In some cases, the conditional expectation $\mathds{E}[\Theta|X]$ may be hard to compute or implement. In that case, we can restrict our attention to estimators of the form $\hat{\Theta}=aX+b.$ Then,
		\begin{align*}
			\hat{\Theta}_\text{LLMS}&=\mathds{E}[\Theta]+\frac{\cov(\Theta,X)}{\var(X)}\left(X-\mathds{E}[X]\right)\\
			&=\mathds{E}[\Theta]+\rho\frac{\sigma_\Theta}{\sigma_X}\left(X-\mathds{E}[X]\right)
		\end{align*}
		\item[Gaussian Distribution] $\mu=-\dfrac{\beta}{2\alpha}, \sigma^2=\dfrac{1}{2\alpha}$
			$$f_X(x) = c e^{-(\alpha x^2 + \beta x + \gamma)}$$
		\item[Bayes Rule]~
			\begin{enumerate}
					\item[]Discrete $\Theta$, Discrete $X$
	
					$$p_{\Theta|X}(\theta|x)=\frac{p_\Theta(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}$$
					$$p_X(x)=\sum_{\theta'}p_\Theta(\theta')p_{X|\Theta}(x|\theta')$$
					\item[]Discrete $\Theta$, Continuous $X$
					
					$$p_{\Theta|X}(\theta|x)=\frac{p_\Theta(\theta)f_{X|\Theta}(x|\theta)}{f_X(x)}$$
					$$f_X(x)=\sum_{\theta'}p_\Theta(\theta')f_{X|\Theta}(x|\theta')$$
					\item[]Continuous $\Theta$, Continuous $X$
					
					$$f_{\Theta|X}(\theta|x)=\frac{f_\Theta(\theta)f_{X|\Theta}(x|\theta)}{f_X(x)}$$
					$$f_X(x)=\int f_\Theta(\theta')f_{X|\Theta}(x|\theta')d\theta'$$
					\item[]Continuous $\Theta$, Discrete $X$
					
					$$f_{\Theta|X}(\theta|x)=\frac{f_\Theta(\theta)p_{X|\Theta}(x|\theta)}{p_X(x)}$$
					$$p_X(x)=\int f_\Theta(\theta')p_{X|\Theta}(x|\theta')d\theta'$$
			\end{enumerate}
		\item[Jeffreys Prior] Gives more weight to values of $\theta$ where
			\begin{enumerate}
				\item MLE estimate has less uncertainty
				\item Data has more information torwards deciding the parameter
				\item Potential outcomes are more sensitive to slight changes in $\theta$
			\end{enumerate}
		$$\pi_J(\theta)\propto\sqrt{\det I(\theta)}$$
		\item{\bf Reparametrization invariance principle}: If $\eta$ is a reparametrization of $\theta$ (i.e., $\eta=\phi(\theta)$ for some one-to-one map $\phi$), then the PDF $\tilde{\pi}(\cdot)$ of $\eta$ satisfies:
	$$\tilde{\pi}(\eta)\propto\sqrt{\det\tilde{I}(\eta)},$$
	where $\tilde{I}(\eta)$ is the Fisher information of the statistical model parametrized by $\eta$ instead of $\theta$.
	
	For $\theta = f(\theta_1)$, $I(\theta)d\theta = I(f(\theta_1))df(\theta_1)=I^{(1)}(\theta_1)d\theta_1$
	\end{description}

\section{Linear Regression}
	\begin{description}
		\item[Regression Function]~
			Give Join Prob Distribution $\mathbb{P}$, the regression function of Y with respect to X is 
			$$v(x) = \mathbb{E}[Y|X=x]=\sum_{\Omega_{Y}}y \mathbb{P}(Y=y|X=x)$$
			$$m(x), \int_{-\infty}^{m(x)}h(y|x)dy=\frac{1}{2}$$
			$$m(x), \int_{-\infty}^{m(x)}h(y|x)dy=1-\alpha$$
		\item[Probabilistic Analysis]~
			\begin{align*}
				b^* & = \dfrac{\cov\left(X,Y\right)}{\var\left(X\right)}\\
				a^* & = \mathbb{E}[Y]-b^*\mathbb{E}[X]=\mathbb{E}[Y]-\dfrac{\cov\left(X,Y\right)}{\var\left(X\right)}\mathbb{E}[X]
			\end{align*}
		\item[LSE]~
				\begin{align*}
					\hat{b} & = \dfrac{\overline{XY}-\overline{X}\,\overline{Y}}{\overline{X^2}-\overline{X}^2}\\
					\hat{a} & = \overline{Y}-\hat{b}\overline{X}
				\end{align*}
		\item[Property of LSE]~
			\begin{itemize}
				\item LSE = MSE
				\item Distribution of $\widehat{\boldsymbol{\beta}}$:
				$$\widehat{\boldsymbol{\beta}}\sim\mathcal{N}_p\left(\boldsymbol{\beta}^*,\sigma^2\left(\mathbb{X}^\intercal\mathbb{X}\right)^{-1}\right)$$
				\item Quadratic Risk of $\widehat{\boldsymbol{\beta}}$:
				$$\mathbb{E}\left[\lVert\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}\rVert_2^2\right]=\sigma^2\textbf{tr}\left(\left(\mathbb{X}^\intercal\mathbb{X}\right)^{-1}\right)$$
				\item Prediction Error:
				$$\mathbb{E}\left[\lVert\mathbf{Y}-\mathbb{X}\widehat{\boldsymbol{\beta}}\rVert_2^2\right]=\sigma^2\left(n-p\right)$$
				\item Unbiased estimator of $\sigma^2$:
				$$\widehat{\sigma}^2=\dfrac{\lVert\mathbf{Y}-\mathbb{X}\widehat{\boldsymbol{\beta}}\rVert_2^2}{n-p}=\dfrac{1}{n-p}\sum_{i=1}^{n}\widehat{\varepsilon}_i^{\,2}$$
				\item Fisher Info $I(\beta) = \dfrac{X^T X}{\sigma^2}$
				\item Heterosckedasticity $\widehat{\beta} = (X^T \Sigma^{-1}X)^{-1}X^T\Sigma^{-1}Y$
			\end{itemize}
		\item[T test (Non-asym)]~
			$$\dfrac{\mu^T \widehat{\beta} - \mu^T \beta^{0}}{\widehat{\sigma} \sqrt{\mu^T(X^T X)^{-1} \mu}} \sim t_{n-p}$$
	\end{description}

\section{Generalized Linear Model}
\begin{description}
	\item[Generalization]~
		\begin{enumerate}
			\item {Random component}: $Y|X=x\sim$ some distribution
			\item {Regression function}: $\left(\mu(x)\right)=x^\intercal\beta$, g is the link function
		\end{enumerate}

	\item[Exponential Family]~
		A family of distribution with the following format
		\begin{itemize}
			\item $\eta_1,\dots,\eta_k$ and $B(\theta)$
			\item $T_1,\dots,T_k$, and $h(y)\in\mathbb{R}^q$
		\end{itemize}
		such that the density function of $\mathbb{P}_\theta$ can be written as
		$$f_\theta(y)=\exp\left[\sum_{i=1}^{k}\eta_i(\theta)T_i(y)-B(\theta)\right]h(y)$$

	\item[One Param Canonical Exponential Family]~
		$$f_\theta(y)=\exp\left(\dfrac{y\theta-b(\theta)}{\phi}+c(y,\phi)\right)$$
		for some known functions $b(\theta)$ and $c(y,\phi)$.
		\begin{itemize}
			\item {\bf Expected value} $\mathbb{E}\left[Y\right]=b'(\theta).$
			\item {\bf Variance} $\var(Y)=b''(\theta)\cdot\phi$
		\end{itemize}
	\item[GLM]~
		\begin{itemize}
			\item {\bf Link function} Relate $X^T \beta$ to $\mu$
				$$X^T\beta = g(\mu) = g(\mu(X))$$
				$$\mu = g^{-1}(X^T \beta)$$
			\item {\bf Canonical Link} Function that link $\mu$ to the canonical parameter $\theta$
				$$g(\mu) = \theta = (b^{'})^{-1}(\mu)$$
			\item {\bf Full Model}
				\begin{description}
					\item Let $(X_i,Y_i)\in\mathbb{R}^p\times\mathbb{R}$, $i=1,\dots,n$ be independent random pairs such that the conditional distribution of $Y_i$ given $X_i=x_i$ has density in the canonical exponential family:
					$$f_{\theta_i}(y_i)=\exp\left[\dfrac{y_i\theta_i-b(\theta_i)}{\phi}+c(y_i,\phi)\right]$$
					\item[Back to $\beta$:] Given a link function $g$, note the following relationship between $\beta$ and $\theta$:
					$$\theta_i=(b')^{-1}(\mu_i)=(b')^{-1}\left(g^{-1}(X_i^\intercal\beta)\right)\equiv h\left(X_i^\intercal\beta\right)$$
					where $h$ is defined as $$h=(b')^{-1}\circ g^{-1}=(g\circ b')^{-1}.$$
					If $g$ is the {\it canonical link function}, $h$ is the {\bf identity} $g=(b')^{-1}$.
					\item[Log-likelihood] The log-likelihood is given by
					\begin{align*}
						\ell_n\left(\mathbf{Y},\mathbb{X},\beta\right) & =\sum_{i}\dfrac{Y_i\theta_i-b(\theta_i)}{\phi}+\text{constant}\\
						& = \sum_{i}\dfrac{Y_ih\left(X_i^\intercal\beta\right)-b\left(h\left(X_i^\intercal\beta\right)\right)}{\phi}+\text{constant}
					\end{align*}
					When we use the {\it canonical link function}, we obtain the expression
					$$\ell_n\left(\mathbf{Y},\mathbb{X},\beta\right)=\sum_{i}\dfrac{Y_iX_i^\intercal\beta-b\left(X_i^\intercal\beta\right)}{\phi}+\text{constant}$$
				\end{description}
		\end{itemize}
\end{description}

\section{Counting}
\begin{enumerate}
\item[\bf Selection] For a selection that can be done in r stages, wight $n_i$ choices at each stage i, the number of possible selection is: $n_1n_2...n-_r$
\item[\bf Permutation] \# of ways of ordering n distinct elements: $n!=1*2*3...n$
\item[\bf Combinations] Give a set of n elements, number of ways of constructing an {\bf ordered} sequence of k {\bf distinct} element (result order does not matter): $\binom{n}{k}=\frac{n!}{k!(n-k)!}$
\item[\bf Subsets] for subset of $\{1,...,n\}$: $\sum_{k=0}^{n}\binom{n}{k}=\binom{n}{0}+\cdots+\binom{n}{n}=2^n.$
\item[\bf Partitions] $\frac{n!}{n_1!n_2!\dots n_r!}.$
\end{enumerate}

\section{Bernoulli Process}
\begin{description}
	\item[Def.] A sequence of Bournoulli trials $X_i$ (independence + time-homogeneity)
	\item[First Arrival] Time of first arrival
		\begin{itemize}
			\item $T_1 = min\{i: X_i=1\}$
			\item $\mathbb{P}(T_1=k) = (1-p)^{k-1}p$
			\item $\mathbb{E}(T_1) = \dfrac{1}{p}$
			\item $var(T_1) = \dfrac{1-p}{p^2}$
		\end{itemize}
	\item[Memoryless] Fresh start after a random time N. $X_{N+1}, X_{N+2}, ...$ is a Bernoulli Process independent of N, $X_1, ... ,X_N$
	\item[K-th Arrival]~
		\begin{itemize}
			\item i-th inter-arrival time $T_i =Y_i - Y_{i-1} \sim Gep(p)$ and independent with $T_j$
			\item $\mathbb{E}[Y_k]=\dfrac{k}{p}, var(Y_k)=\dfrac{k(1-p)}{p^2}$
			\item $\mathbb{P}_{Y_k}= \binom{t-1}{k-1}p^k(1-p)^{t-k}$
		\end{itemize}
	\item[Possion appro] Total number of arrivals converge to Poisson distribution for large n, small p and moderate $\lambda=np$
\end{description}

\section{Poisson Process}
\begin{description}
	\item[Prob of k arrivals in duration $\delta$] ($\lambda$ is arrival rate, $\tau = n \delta$, $N_{\tau}$ is binomial and converge to Poisson) Then $\mathbb{P}(k, \delta)=$
		$$\begin{cases}
				1 - \lambda \delta + O(\delta^2) \qquad\text{if k=0}\\[10pt]
				\lambda \delta + O(\delta^2)\qquad \text{if k=1}\\[10pt]
				0 + O(\delta^2) \qquad \text{if k$>$1}
		\end{cases}$$

		$$\mathbb{P}(k,\tau) = \mathbb{P}(N_{\tau}) = \dfrac{(\lambda \tau)^k e^{-\lambda \tau}}{k!}$$

	\item[Time until first arrival] $T_1 \sim Exp(\lambda)$

	\item[Time $Y_k$ of the kth arrival]~
		\begin{itemize}
			\item Sum of independent Exp $Y_k = T_1 + T_2 + ... + T_k$
			\item $\mathbb{P}(Y_k \leq y) = \sum_{n=k}^{\infty}\mathbb{P}(n, y)$
			\item $Y_k \sim Erlang(k),  f_{Y_k}(y) = \dfrac{\lambda^k y^{k-1} e^{-\lambda y} }{(k-1)!}$
			\item $\mathbb{E}[Y_k] = \dfrac{k}{\lambda}, var(Y_k)=\dfrac{k}{\lambda^2}$
		\end{itemize}
	\item[Memoryless]~
		Starting from a constant time t or a certain arrival $T_k$ (k is a constant), the following is a poisson process independent with history. Can divide time line and conque separately
	\item[Merge]~
		\begin{itemize}
			\item Sum of two Poisson process is a poisson process of parameter $\mu + v$
			\item $\mathbb{P}(kth - 1st) = \dfrac{\lambda_1}{\lambda_1 + \lambda_2}$
			\item k of N arrivals are first = $\binom{N}{k}(\dfrac{\lambda_1}{\lambda_1 + \lambda_2})^k (\dfrac{\lambda_2}{\lambda_1 + \lambda_2})^{N-k}$
			\item Assume X, Y, Z are time until first arrival of 3 poisson process $\min(X, Y, Z) \sim Poisson(\lambda_1 + \lambda_2 + \lambda_3)$
		\end{itemize}
	\item[Split]~
		A poisson process can be splited in to Poisson($\lambda q$) and Poisson($\lambda (1-q)$). And the resulting 2 proceses are independent
	\item[Random Incidence]~ 
		Arrival at constant $t^{\star}$ U, V are time of last and next arrival.
		$$(V - t^{\star}) , (t^{\star} - U) \sim Exp(\lambda)$$
\end{description}


\section{Markov Process}
Given curent state, past does not matter
\begin{description}
	\item[N step transition prob]~
		\begin{itemize}
			\item $r_{ij}(n) = \mathbb{P}(X_{n+s}=j | X_s =i)$
			\item recursion: $r_{ij}(n) = \sum_{k=1}^{m}r_{ik}(n-1)p_{kj}$
			\item random initial state: $\mathbb{P}(X_n=j) = \sum_{i=1}^{m}\mathbb{P}(X_0=i)r_{ij}(n)$
			\item convergence to $\pi_{j}$ (not depend on n and i; only one recurrent class and it is not periodic)
		\end{itemize}
	\item[Recurrent]~
		\begin{itemize}
			\item  States: starting from i, and from wherever you can go, there is a way of returning to i
			\item  Class: a collection of recurrent states communicating only between each other
			\item Periodic states in recurrent class:can be grouped in to d$>$1 groups so that all transitions from one group lead to the next group 
		\end{itemize}
	\item[Steady-stae Prob]~
		\begin{itemize}
			\item Converge to $\pi_j$ if recurrent states are all in single class and is not periodic
			\item $\pi_j = \sum_k \pi_k p_{kj},   \sum_{j=1}^m\pi_j = 1$
			\item can be interpreted as: long run frequency in j, frequency of transition intoj
		\end{itemize}
	\item[birth-death process]~
		\begin{itemize}
			\item $\pi_i p_i = \pi_{i+1}q_{i+1}$
			\item $\pi_0 + \pi_0 \dfrac{p_0}{q_1} + \pi_0 \dfrac{p_0 p_1}{q_1 q_2} + ... = 1$
			\item For fixed p $<$q:  $\mathbb{E}(X_n)[\dfrac{\rho}{1-\rho}]$
			\item for p=q, all $\pi$ equal
		\end{itemize}
	\item[Absorption state]~
		\begin{itemize}
			\item $a_i$ is the prob that eventually settle in absorb state a starting from i $a_i = \sum_{j=1}^m p_{ij}a_j$
			\item $\mu_i$ is the expected number of transitions reaching absorb state a starting from i $\mu_i = 1 + \sum_j p_{ij}\mu_j$
		\end{itemize}
	\item[First passage and recurrence times]~
		\begin{itemize}
			\item Mean first passage time from i to s: $t_i = \mathbb{E}[min\{n \geq 0 such X_n=s\}|X_0=i]$ 
			\item $t_s =0,  t_i = 1 + \sum_j p_{ij}t_j$ for all $i <> s$
			\item $t_s^{\star} = \mathbb{E}[min\{n \geq 1 such X_n=s\}|X_0=s]$ 
			\item $t_s^{\star} = 1 + \sum_j p_{sj}t_j$
		\end{itemize}
\end{description}

\section{Derived Distribution}
Given distribution of x what is the distribution of $y = g(x)$
\begin{description}
	\item[Discrete case]~
		\begin{itemize}
			\item $\mathbb{P}_Y(y) = \mathbb{P}(g(x) = y) = \sum_{x: g(x)=y}\mathbb{P}_X(x)$
			\item When $g(x) = ax + b$  $$\mathbb{P}_Y(y)=\mathbb{P}_X(\dfrac{y-b}{a})$$
		\end{itemize}
	\item[Continuous case]~
		\begin{itemize}
			\item {\bf CDF}: $F_Y(y)=\mathbb{P}(Y \leq y) = \mathbb{P}(g(x) \leq y)$
			\item {\bf Take derivative} $f_Y(y) = \dfrac{dF_Y}{dy}(y)$
			\item when $g(X) = aX +b$, then $f_Y(y) = \dfrac{1}{|a|}f_X(\dfrac{y-b}{a})$
			\item when g is mononic. $$f_Y(y) = f_X(g^{-1}(y)) |\dfrac{g^{-1}(y)}{dy}|$$
		\end{itemize}
	\item[Sum of RV]~
		\begin{itemize}
			\item Z=X+Y, $P_Z(z) = \sum_x P_X(x)P_Y(z-x)$
			\item $f_Z(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z-x)dx$
			\item $var(X_1+...+X_n) = \sum_i^n var(X_i) + \sum_{\{(i,j):i<>j\}}cov(X_i, X_j)$
			\item for $Y = X_1 + ... + X_N$ where N is also a RV
				$$E[Y] = E[N]E[X]$$
				$$Var[Y] = E[N]var[X] + (E[X])^2var(N)$$
		\end{itemize}



\end{description}


\end{multicols*}
\end{document}